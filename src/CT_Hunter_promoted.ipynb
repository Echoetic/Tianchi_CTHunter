{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2d559d",
   "metadata": {},
   "source": [
    "## **题目3：CT猎人：医学影像报告异常检测（改进版）**\n",
    "#### 一、导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634c52b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef5012",
   "metadata": {},
   "source": [
    "#### 二、设置日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0311679",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dec3b8",
   "metadata": {},
   "source": [
    "#### 三、设置随机种子以确保可复现性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6884135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f45410",
   "metadata": {},
   "source": [
    "#### 四、设置设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02681cc",
   "metadata": {},
   "source": [
    "#### 五、数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae36f75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTDataset(Dataset):\n",
    "    def __init__(self, descriptions, labels=None, max_length=None):\n",
    "        self.descriptions = descriptions\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.descriptions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        desc = self.descriptions[idx]\n",
    "        # 如果设置了最大长度，进行截断\n",
    "        if self.max_length and len(desc) > self.max_length:\n",
    "            desc = desc[:self.max_length]\n",
    "            \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return desc, label\n",
    "        return desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68323714",
   "metadata": {},
   "source": [
    "#### 六、用FocalLoss类处理类别不平衡问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053063e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bd1f9c",
   "metadata": {},
   "source": [
    "#### 七、添加了注意力机制后改进的LSTM分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedCTClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, \n",
    "                 dropout_rate, num_classes, use_attention=True):\n",
    "        super(ImprovedCTClassifier, self).__init__()\n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                           batch_first=True, bidirectional=True, \n",
    "                           dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        lstm_output_dim = hidden_dim * 2  # 双向LSTM\n",
    "        \n",
    "        if self.use_attention:\n",
    "            self.attention = nn.MultiheadAttention(lstm_output_dim, num_heads=8, \n",
    "                                                 dropout=dropout_rate, batch_first=True)\n",
    "            \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # 添加批归一化\n",
    "        self.batch_norm = nn.BatchNorm1d(lstm_output_dim)\n",
    "        \n",
    "        # 多层分类头\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_output_dim, lstm_output_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(lstm_output_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 初始化权重\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'embedding' in name:\n",
    "                nn.init.normal_(param, mean=0, std=0.1)\n",
    "            elif 'lstm' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "            elif 'linear' in name or 'classifier' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "        \n",
    "    def forward(self, x, attention_mask):\n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        # 嵌入层\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        embedded = embedded * attention_mask.unsqueeze(2)\n",
    "        \n",
    "        # LSTM层\n",
    "        lstm_out, _ = self.lstm(embedded)  # [batch_size, seq_len, hidden_dim*2]\n",
    "        \n",
    "        if self.use_attention:\n",
    "            # 自注意力机制\n",
    "            # 创建key_padding_mask (True表示要被忽略的位置)\n",
    "            key_padding_mask = (1.0 - attention_mask).bool()\n",
    "            \n",
    "            attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out, \n",
    "                                       key_padding_mask=key_padding_mask)\n",
    "            \n",
    "            # 残差连接\n",
    "            lstm_out = lstm_out + attn_out\n",
    "        \n",
    "        # 加权平均池化（考虑注意力掩码）\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand_as(lstm_out)\n",
    "        masked_output = lstm_out * mask_expanded\n",
    "        sum_embeddings = masked_output.sum(1)  # [batch_size, hidden_dim*2]\n",
    "        sum_mask = mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1.0)\n",
    "        pooled_output = sum_embeddings / sum_mask\n",
    "        \n",
    "        # 批归一化\n",
    "        pooled_output = self.batch_norm(pooled_output)\n",
    "        \n",
    "        # 分类头\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return torch.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71433b3d",
   "metadata": {},
   "source": [
    "#### 八、改进的Transformer分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf809002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedTransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, nhead, num_layers, \n",
    "                 dim_feedforward, dropout_rate, num_classes):\n",
    "        super(ImprovedTransformerClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1000, embedding_dim) * 0.1)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-norm架构，通常更稳定\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # 多层分类头\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim // 2),\n",
    "            nn.GELU(),  # 使用GELU激活函数\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(embedding_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'embedding' in name:\n",
    "                nn.init.normal_(param, mean=0, std=0.1)\n",
    "            elif 'linear' in name or 'classifier' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "        \n",
    "    def forward(self, x, attention_mask):\n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        # 嵌入 + 位置编码\n",
    "        embedded = self.embedding(x)\n",
    "        if seq_len <= self.pos_encoding.size(0):\n",
    "            embedded = embedded + self.pos_encoding[:seq_len].unsqueeze(0)\n",
    "        \n",
    "        # Transformer注意力掩码\n",
    "        transformer_mask = (1.0 - attention_mask).bool()\n",
    "        \n",
    "        # Transformer编码器\n",
    "        transformer_out = self.transformer_encoder(embedded, src_key_padding_mask=transformer_mask)\n",
    "        \n",
    "        # 全局平均池化\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand_as(transformer_out)\n",
    "        masked_output = transformer_out * mask_expanded\n",
    "        sum_embeddings = masked_output.sum(1)\n",
    "        sum_mask = mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1.0)\n",
    "        pooled_output = sum_embeddings / sum_mask\n",
    "        \n",
    "        # 层归一化\n",
    "        pooled_output = self.layer_norm(pooled_output)\n",
    "        \n",
    "        # 分类头\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return torch.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174535fb",
   "metadata": {},
   "source": [
    "#### 九、改进的数据加载函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5000e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path, test_path=None):\n",
    "    try:\n",
    "        # 加载训练数据\n",
    "        col_names = ['report_ID', 'description', 'label']\n",
    "        train_df = pd.read_csv(train_path, sep=r'\\|,\\|', header=None, names=col_names, \n",
    "                              engine='python', dtype=str)\n",
    "        \n",
    "        # 处理描述列\n",
    "        train_descriptions = []\n",
    "        for desc in train_df['description'].values:\n",
    "            if pd.isna(desc) or desc.strip() == '':\n",
    "                tokens = []\n",
    "            else:\n",
    "                try:\n",
    "                    tokens = list(map(int, desc.strip().split()))\n",
    "                except ValueError:\n",
    "                    tokens = []\n",
    "                    logger.warning(f\"无法解析描述: {desc}\")\n",
    "            train_descriptions.append(tokens)\n",
    "        \n",
    "        # 处理标签列\n",
    "        train_labels = np.zeros((len(train_df), 17))\n",
    "        for i, label_str in enumerate(train_df['label'].values):\n",
    "            if pd.notna(label_str) and label_str.strip() != '':\n",
    "                try:\n",
    "                    for label in str(label_str).strip().split():\n",
    "                        if label.isdigit():\n",
    "                            label_idx = int(label)\n",
    "                            if 0 <= label_idx < 17:\n",
    "                                train_labels[i, label_idx] = 1\n",
    "                except ValueError:\n",
    "                    logger.warning(f\"无法解析标签: {label_str}\")\n",
    "        \n",
    "        # 统计信息\n",
    "        seq_lengths = [len(desc) for desc in train_descriptions if desc]\n",
    "        vocab_size = 1\n",
    "        if train_descriptions:\n",
    "            all_tokens = [token for desc in train_descriptions if desc for token in desc]\n",
    "            if all_tokens:\n",
    "                vocab_size = max(all_tokens) + 1\n",
    "        \n",
    "        max_seq_len = max(seq_lengths) if seq_lengths else 0\n",
    "        \n",
    "        logger.info(f\"训练集大小: {len(train_df)}\")\n",
    "        if seq_lengths:\n",
    "            logger.info(f\"序列长度统计 - 平均: {np.mean(seq_lengths):.2f}, \"\n",
    "                       f\"最大: {max_seq_len}, 最小: {min(seq_lengths)}\")\n",
    "            logger.info(f\"序列长度分位数 - 50%: {np.percentile(seq_lengths, 50):.0f}, \"\n",
    "                       f\"75%: {np.percentile(seq_lengths, 75):.0f}, \"\n",
    "                       f\"90%: {np.percentile(seq_lengths, 90):.0f}\")\n",
    "        logger.info(f\"词汇表大小: {vocab_size}\")\n",
    "        \n",
    "        # 标签分布统计\n",
    "        label_dist = train_labels.sum(axis=0)\n",
    "        logger.info(\"各区域异常标签分布:\")\n",
    "        for i, count in enumerate(label_dist):\n",
    "            logger.info(f\"区域 {i}: {count} 例 ({count/len(train_df)*100:.2f}%)\")\n",
    "        \n",
    "        # 加载测试数据\n",
    "        test_descriptions = None\n",
    "        test_report_ids = None\n",
    "        if test_path:\n",
    "            test_df = pd.read_csv(test_path, sep=r'\\|,\\|', header=None, names=col_names, \n",
    "                                 engine='python', dtype=str)\n",
    "            test_report_ids = test_df['report_ID'].values\n",
    "            \n",
    "            test_descriptions = []\n",
    "            for desc in test_df['description'].values:\n",
    "                if pd.isna(desc) or str(desc).strip() == '':\n",
    "                    tokens = []\n",
    "                else:\n",
    "                    try:\n",
    "                        tokens = list(map(int, str(desc).strip().split()))\n",
    "                    except ValueError:\n",
    "                        tokens = []\n",
    "                        logger.warning(f\"无法解析测试描述: {desc}\")\n",
    "                test_descriptions.append(tokens)\n",
    "            \n",
    "            logger.info(f\"测试集大小: {len(test_df)}\")\n",
    "        \n",
    "        return train_descriptions, train_labels, test_descriptions, test_report_ids, vocab_size, max_seq_len\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"数据加载失败: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa6812e",
   "metadata": {},
   "source": [
    "#### 十、改进的批处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, max_length=None):\n",
    "    if isinstance(batch[0], tuple):  # 训练模式\n",
    "        descriptions = [item[0] for item in batch]\n",
    "        labels = torch.tensor([item[1] for item in batch], dtype=torch.float32)\n",
    "    else:  # 测试模式\n",
    "        descriptions = batch\n",
    "        labels = None\n",
    "    \n",
    "    # 计算批次中的最大长度\n",
    "    if descriptions and any(desc for desc in descriptions):\n",
    "        max_len = max(len(desc) for desc in descriptions if desc)\n",
    "    else:\n",
    "        max_len = 1\n",
    "    \n",
    "    # 如果设置了最大长度限制\n",
    "    if max_length:\n",
    "        max_len = min(max_len, max_length)\n",
    "    \n",
    "    # 填充序列\n",
    "    padded_descs = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for desc in descriptions:\n",
    "        # 截断或填充\n",
    "        if len(desc) > max_len:\n",
    "            desc = desc[:max_len]\n",
    "        \n",
    "        padded_desc = desc + [0] * (max_len - len(desc))\n",
    "        attention_mask = [1] * len(desc) + [0] * (max_len - len(desc))\n",
    "        \n",
    "        padded_descs.append(padded_desc)\n",
    "        attention_masks.append(attention_mask)\n",
    "    \n",
    "    padded_descs = torch.tensor(padded_descs, dtype=torch.long)\n",
    "    attention_masks = torch.tensor(attention_masks, dtype=torch.float32)\n",
    "    \n",
    "    if labels is not None:\n",
    "        return padded_descs, attention_masks, labels\n",
    "    else:\n",
    "        return padded_descs, attention_masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd44525",
   "metadata": {},
   "source": [
    "#### 十一、计算多种评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefa2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(labels, predictions, threshold=0.5):\n",
    "    metrics = {}\n",
    "    \n",
    "    # AUC分数\n",
    "    aucs = []\n",
    "    for i in range(labels.shape[1]):\n",
    "        if np.sum(labels[:, i]) > 0 and np.sum(labels[:, i]) < len(labels):\n",
    "            auc = roc_auc_score(labels[:, i], predictions[:, i])\n",
    "            aucs.append(auc)\n",
    "    \n",
    "    metrics['mean_auc'] = np.mean(aucs) if aucs else 0\n",
    "    metrics['aucs'] = aucs\n",
    "    \n",
    "    # 基于阈值的指标\n",
    "    binary_preds = (predictions > threshold).astype(int)\n",
    "    \n",
    "    # F1分数\n",
    "    f1_scores = []\n",
    "    for i in range(labels.shape[1]):\n",
    "        if np.sum(labels[:, i]) > 0:  # 只计算有正样本的类别\n",
    "            f1 = f1_score(labels[:, i], binary_preds[:, i], zero_division=0)\n",
    "            f1_scores.append(f1)\n",
    "    \n",
    "    metrics['mean_f1'] = np.mean(f1_scores) if f1_scores else 0\n",
    "    metrics['f1_scores'] = f1_scores\n",
    "    \n",
    "    # 平均精度\n",
    "    avg_precisions = []\n",
    "    for i in range(labels.shape[1]):\n",
    "        if np.sum(labels[:, i]) > 0:\n",
    "            ap = average_precision_score(labels[:, i], predictions[:, i])\n",
    "            avg_precisions.append(ap)\n",
    "    \n",
    "    metrics['mean_ap'] = np.mean(avg_precisions) if avg_precisions else 0\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b18ab76",
   "metadata": {},
   "source": [
    "#### 十二、改进的训练函数，添加早停和更好的监控"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853f300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, \n",
    "                num_epochs, model_path, patience=5):\n",
    "    best_val_auc = 0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_aucs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds_list = []\n",
    "        train_labels_list = []\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            inputs, attention_masks, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds_list.append(outputs.detach().cpu().numpy())\n",
    "            train_labels_list.append(labels.detach().cpu().numpy())\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # 计算训练集指标\n",
    "        train_preds = np.vstack(train_preds_list)\n",
    "        train_labels = np.vstack(train_labels_list)\n",
    "        train_metrics = compute_metrics(train_labels, train_preds)\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds_list = []\n",
    "        val_labels_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "            for batch in progress_bar:\n",
    "                inputs, attention_masks, labels = batch\n",
    "                inputs = inputs.to(device)\n",
    "                attention_masks = attention_masks.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs, attention_masks)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_preds_list.append(outputs.cpu().numpy())\n",
    "                val_labels_list.append(labels.cpu().numpy())\n",
    "                \n",
    "                progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 计算验证集指标\n",
    "        val_preds = np.vstack(val_preds_list)\n",
    "        val_labels = np.vstack(val_labels_list)\n",
    "        val_metrics = compute_metrics(val_labels, val_preds)\n",
    "        val_aucs.append(val_metrics['mean_auc'])\n",
    "        \n",
    "        # 学习率调度\n",
    "        if scheduler:\n",
    "            scheduler.step(val_metrics['mean_auc'])\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        logger.info(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        logger.info(f'  Train Loss: {train_loss}, Train AUC: {train_metrics[\"mean_auc\"]}')\n",
    "        logger.info(f'  Val Loss: {val_loss}, Val AUC: {val_metrics[\"mean_auc\"]}, Val F1: {val_metrics[\"mean_f1\"]}')\n",
    "        logger.info(f'  Current Learning Rate: {current_lr}')\n",
    "\n",
    "        # 早停和模型保存\n",
    "        if val_metrics['mean_auc'] > best_val_auc:\n",
    "            best_val_auc = val_metrics['mean_auc']\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'best_val_auc': best_val_auc,\n",
    "                'val_metrics': val_metrics\n",
    "            }, model_path)\n",
    "            logger.info(f'新的最佳模型已保存到 {model_path}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            logger.info(f'验证集AUC连续{patience}轮未改善，提前停止训练')\n",
    "            break\n",
    "    \n",
    "    logger.info(f'训练完成！最佳验证AUC: {best_val_auc} (第{best_epoch+1}轮)')\n",
    "    \n",
    "    # 绘制训练曲线\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(val_aucs, label='Validation AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    plt.title('Validation AUC')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    epochs_range = range(len(train_losses))\n",
    "    train_aucs = [compute_metrics(np.vstack(train_labels_list), np.vstack(train_preds_list))['mean_auc'] \n",
    "                  for _ in epochs_range]\n",
    "    plt.plot(epochs_range, train_aucs, label='Train AUC')\n",
    "    plt.plot(epochs_range, val_aucs, label='Validation AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    plt.title('Train vs Validation AUC')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png', dpi=300)\n",
    "    logger.info('训练曲线已保存到 training_curves.png')\n",
    "    \n",
    "    return best_val_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ad0e09",
   "metadata": {},
   "source": [
    "#### 十三、预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='预测中'):\n",
    "            inputs, attention_masks = batch\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            \n",
    "            outputs = model(inputs, attention_masks)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c343a85a",
   "metadata": {},
   "source": [
    "#### 十四、参数类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8091dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    train = 'combined_train_data.csv'\n",
    "    test = 'track1_round1_testB.csv'\n",
    "    model = 'lstm'  # 可选: 'lstm', 'transformer'\n",
    "    batch_size = 32\n",
    "    epochs = 20\n",
    "    lr = 0.00139\n",
    "    embedding_dim = 256\n",
    "    hidden_dim = 256\n",
    "    num_layers = 3\n",
    "    dropout = 0.3\n",
    "    output = 'predictions_improved.csv'\n",
    "    nhead = 8\n",
    "    dim_feedforward = 512\n",
    "    max_length = 150\n",
    "    loss_type = 'focal'  # or 'bce'\n",
    "    patience = 6\n",
    "    seed = 42\n",
    "    use_attention = True\n",
    "    use_scheduler = False\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd1bb6",
   "metadata": {},
   "source": [
    "#### 十五、主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfba923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # 设置随机种子\n",
    "    set_seed(args.seed)\n",
    "    \n",
    "    # 加载数据\n",
    "    train_descriptions, train_labels, test_descriptions, test_report_ids, vocab_size, max_seq_len = load_data(\n",
    "        args.train, args.test)\n",
    "    \n",
    "    # 使用设定的最大长度或数据中的最大长度\n",
    "    actual_max_length = min(args.max_length, max_seq_len) if max_seq_len > 0 else args.max_length\n",
    "    logger.info(f\"使用最大序列长度: {actual_max_length}\")\n",
    "    \n",
    "    # 划分训练集和验证集\n",
    "    val_size = int(0.2 * len(train_descriptions))\n",
    "    indices = list(range(len(train_descriptions)))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_indices = indices[val_size:]\n",
    "    val_indices = indices[:val_size]\n",
    "    \n",
    "    train_descs = [train_descriptions[i] for i in train_indices]\n",
    "    train_labs = train_labels[train_indices]\n",
    "    val_descs = [train_descriptions[i] for i in val_indices]\n",
    "    val_labs = train_labels[val_indices]\n",
    "\n",
    "    # 创建数据集和数据加载器\n",
    "    train_dataset = CTDataset(train_descs, train_labs, max_length=actual_max_length)\n",
    "    val_dataset = CTDataset(val_descs, val_labs, max_length=actual_max_length)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, \n",
    "                             collate_fn=lambda x: collate_fn(x, actual_max_length))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, \n",
    "                           collate_fn=lambda x: collate_fn(x, actual_max_length))\n",
    "\n",
    "    # 创建模型\n",
    "    num_classes = 17\n",
    "    \n",
    "    if args.model == 'lstm':\n",
    "        model = ImprovedCTClassifier(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=args.embedding_dim,\n",
    "            hidden_dim=args.hidden_dim,\n",
    "            num_layers=args.num_layers,\n",
    "            dropout_rate=args.dropout,\n",
    "            num_classes=num_classes,\n",
    "            use_attention=args.use_attention  # 使用布尔参数\n",
    "        )\n",
    "        # 根据注意力机制使用情况命名模型文件\n",
    "        model_path = f'improved_ct_lstm_{\"att\" if args.use_attention else \"noatt\"}_model.pth'\n",
    "    else:\n",
    "        model = ImprovedTransformerClassifier(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=args.embedding_dim,\n",
    "            nhead=args.nhead,\n",
    "            num_layers=args.num_layers,\n",
    "            dim_feedforward=args.dim_feedforward,\n",
    "            dropout_rate=args.dropout,\n",
    "            num_classes=num_classes\n",
    "        )\n",
    "        model_path = 'improved_ct_transformer_model.pth'\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    # 统计模型参数\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logger.info(f'模型参数统计 - 总数: {total_params:,}, 可训练: {trainable_params:,}')\n",
    "    \n",
    "    # 优化器\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-5)\n",
    "    \n",
    "    # 学习率调度器 - 根据参数决定是否使用\n",
    "    if args.use_scheduler:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, \n",
    "                                                        patience=3)\n",
    "        logger.info(f'使用学习率调度器: ReduceLROnPlateau')\n",
    "    else:\n",
    "        scheduler = None\n",
    "        logger.info(f'使用固定学习率: {args.lr}')\n",
    "    \n",
    "    # 损失函数\n",
    "    if args.loss_type == 'focal':\n",
    "        criterion = FocalLoss(alpha=1, gamma=2)\n",
    "    else:\n",
    "        criterion = nn.BCELoss()\n",
    "    \n",
    "    logger.info(f'使用损失函数: {args.loss_type}')\n",
    "    logger.info(f'模型类型: {args.model}')\n",
    "    if args.model == 'lstm':\n",
    "        logger.info(f'LSTM注意力机制: {\"启用\" if args.use_attention else \"禁用\"}')\n",
    "    \n",
    "    # 训练模型\n",
    "    logger.info('开始训练模型...')\n",
    "    best_val_auc = train_model(model, train_loader, val_loader, optimizer, criterion, \n",
    "                               scheduler, args.epochs, model_path, patience=args.patience)\n",
    "    \n",
    "    # 加载最佳模型进行预测\n",
    "    if test_descriptions is not None:\n",
    "        logger.info('加载最佳模型进行预测...')\n",
    "        checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # 创建测试数据集和数据加载器\n",
    "        test_dataset = CTDataset(test_descriptions, max_length=actual_max_length)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, \n",
    "                                collate_fn=lambda x: collate_fn(x, actual_max_length))\n",
    "        \n",
    "        # 进行预测\n",
    "        predictions = predict(model, test_loader)\n",
    "        \n",
    "        # 保存预测结果\n",
    "        # 1. 将17维概率向量转换为单个空格分隔的字符串\n",
    "        probability_strings = []\n",
    "        for i in range(predictions.shape[0]):  # predictions 是一个 (n_samples, 17) 的 NumPy 数组\n",
    "            # 将当前行的17个浮点数概率转换为字符串，并用空格连接\n",
    "            current_prob_str = ' '.join([str(p) for p in predictions[i, :]])\n",
    "            probability_strings.append(current_prob_str)\n",
    "            \n",
    "        # 2. 创建一个包含两列的 DataFrame：report_ID 和格式化后的概率字符串\n",
    "        output_df = pd.DataFrame({\n",
    "            'report_ID': test_report_ids,\n",
    "            'probabilities': probability_strings\n",
    "        })\n",
    "        \n",
    "        # 3. 保存到 CSV，不带表头，不带索引，使用 '|,|' 作为分隔符\n",
    "        output_df.to_csv(args.output, header=False, index=False, sep='|')\n",
    "        \n",
    "        logger.info(f'预测结果已保存到 {args.output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09867674",
   "metadata": {},
   "source": [
    "#### 十五、运行函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8279e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_Env",
   "language": "python",
   "name": "you_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
